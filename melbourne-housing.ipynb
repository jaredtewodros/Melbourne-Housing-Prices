#### Introductory Steps

!pip install --upgrade -q gspread
!pip install gspread-dataframe

from google.colab import auth
auth.authenticate_user()

import gspread
from oauth2client.client import GoogleCredentials
from gspread_dataframe import get_as_dataframe, set_with_dataframe

gc = gspread.authorize(GoogleCredentials.get_application_default())

worksheet = gc.open('Melbourne_housing_FULL').sheet1

# get_all_values gives a list of rows.
rows = worksheet.get_all_values()
print(rows)

# Convert to a DataFrame and render.
import pandas as pd
Melbourne = pd.DataFrame.from_records(rows)

#Importing numpy and pandas into google colab
import numpy as np
import pandas as pd

#### Data Cleaning

# In order to understand why there are errors, it is important to look at the variable types
Melbourne.dtypes

# To further do any testing of the data, the variables need to be numeric, allowing for easy manipulation of data
Melbourne = Melbourne.convert_objects(convert_numeric=True)

# Esure that the headers of the data frame are the same as the
# one that are in csv
Melbourne.columns = ('Suburb', 'Address',	'Rooms', 'Type', 'Price',	'Method',
                     'SellerG',	'Date',	'Distance',	'Postcode',	'Bedroom2',
                     'Bathroom',	'Car',	'Landsize',	'BuildingArea',
                     'YearBuilt',	'CouncilArea',	'Lattitude',	'Longtitude',
                     'Regionname',	'Propertycount')
print(Melbourne.columns)

# convert housing types into numbers

Melbourne.loc[Melbourne["Type"]=='h', 'Type'] = 0
Melbourne.loc[Melbourne["Type"]=='t', 'Type'] = 1
Melbourne.loc[Melbourne["Type"]=='u', 'Type'] = 2

# Need to remove the first column since already have row names
Melbourne = Melbourne.iloc[1:]

# Exploring the data using the head() function to further investigate
print(Melbourne.head())

# Providing a summary statistic of all the numerical varibles 
print(Melbourne.describe())

# Then, to clean up the data, these columns must be dropped
Melbourne = Melbourne.drop(columns = ["Suburb", "Address", "Method", "Postcode", 
                                     "SellerG", "Date", "Bedroom2", "CouncilArea", 
                                     "Lattitude", "Longtitude", "Regionname"])

# drop NAs since that we skew house prices
Melbourne = Melbourne.dropna(axis= 'rows', how='any')

# it wouldn't make sense for a house to have no landsize, so the houses with 
# 0/0.0 are removed
Melbourne = Melbourne[Melbourne.Landsize!=0.0]

# checks for missing values after cleaning is finished
total = Melbourne.isnull().sum().sort_values(ascending=False)
percent = (Melbourne.isnull().sum()/Melbourne.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data

#### Making the Model

%matplotlib inline 
import matplotlib.pyplot as plt
Melbourne.hist(bins=50, figsize=(20,15))
plt.show()

from scipy.stats import pearsonr
import matplotlib.pyplot as plt

#Create two random variable 
data = Melbourne
prices = data['Price']
variables = data['Dis']

#plot the variables to show linearity
plt.scatter(variables,prices)
plt.show()

corr_matrix = Melbourne.corr()

# Importing the necessary libraries for the model making
import numpy as np
import pandas as pd
from sklearn.model_selection import ShuffleSplit


# In order to make it more pleasing for the notebook
%matplotlib inline

# Loading the Melbourne housing data to make prediction model
data = Melbourne
prices = data['Price']
variables = data.drop('Price', axis = 1)
    
# In order to tell whether or not the data was input correctly, this code was printed
print("Melbourne housing dataset has {} data points with {} variables each.".format(*data.shape))

# All of these are summary statistics of the Melbourne Housing Data
# Minimum and maximum price of Melbourne
minimum_price = np.amin(prices)
maximum_price = np.amax(prices)

# Mean price
mean_price = np.mean(prices)

# Median price
median_price = np.median(prices)

# Standard deviation of prices of the data
std_price = np.std(prices)

# Printing out the summary statistics that allows us to see how the data is distributed
print("Statistics for Melbourne housing dataset:\n")
print("Minimum price: ${}".format(minimum_price)) 
print("Maximum price: ${}".format(maximum_price))
print("Mean price: ${}".format(mean_price))
print("Median price ${}".format(median_price))
print("Standard deviation of prices: ${}".format(std_price))

# Import 'r2_score' to see how much error arises within the model

from sklearn.metrics import r2_score

def performance_metric(y_true, y_predict):
    """ Calculates and returns the performance score between 
        true (y_true) and predicted (y_predict) values based on the metric chosen. """
    
    score = r2_score(y_true, y_predict)
    
    # Return the score
    return score
    
# Import 'train_test_split' to further performt the training and testing subsets
from sklearn.model_selection import train_test_split

# Shuffle and split the data into training and testing subsets
# train_test_split(x, y, test_size, train_size, random_state)
X_train, X_test, y_train, y_test = train_test_split(variables, prices, test_size=0.2, random_state = 50)

# Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'to reproduce the best model
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV

def fit_model(X, y):
    """ Performs grid search over the 'max_depth' parameter for a 
        decision tree regressor trained on the input data [X, y]. """
    
    # Create cross-validation sets from the training data
    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)
    
    # Create a decision tree regressor object
    regressor = DecisionTreeRegressor()

    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10
    params = {'max_depth':[1,2,3,4,5,6,7,8,9,10]}

    # Transform 'performance_metric' into a scoring function using 'make_scorer' 
    scoring_fnc = make_scorer(performance_metric)

    # Create the grid search cv object --> GridSearchCV()
    grid = GridSearchCV(estimator=regressor, param_grid=params, scoring=scoring_fnc, cv=cv_sets)

    # Fit the grid search object to the data to compute the optimal model
    grid = grid.fit(X, y)

    # Return the optimal model after fitting the data
    return grid.best_estimator_
    
# Fit the training data to the model using grid search
reg = fit_model(X_train, y_train)

# Produce the value for 'max_depth'
print("Parameter 'max_depth' is {} for the optimal model.".format(reg.get_params()['max_depth']))

client_data = [[2,0,2.5,1,0,156.0,79.0,1995.0,4019.0], 
              [5.0, 0, 14.7, 2.0, 4.0, 580.0, 218.0, 1970.0, 16166.0]]

# Show predictions
for i, price in enumerate(reg.predict(client_data)):
    print("Predicted selling price for Client {}'s home: ${:,.2f}".format(i+1, price))
    
# In order to see the errors, the regressor variable needs to be made, which is pbtained from the training model
# then, the prediction is obtained from the testing variables shown
from sklearn.tree import DecisionTreeRegressor  
regressor = DecisionTreeRegressor()  
regressor.fit(X_train, y_train)  
y_pred = regressor.predict(X_test)

# To evaluate the model algorithm:
from sklearn import metrics  
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
